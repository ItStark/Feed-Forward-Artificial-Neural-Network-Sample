#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <alloc.h>
#include <time.h>
#include <conio.h>

#define LIMIT 22
#define ITERATION 1E6

#define ETA 0.010
#define MOMENTUM 0.600
#define GAMMA 0.0001
#define BETA 0.050

#define d(i, j)    *(po + ((j)-1)*LIMIT+(i))
#define x(i, j)    *(pi + ((j)-1)*LIMIT+(i))

#define TOLERANCE 0.000001

//FUNCTIONS

void getStructureInfo(void);
void assignInitialValues(void);
void computeForwardValues(void);
void loadTrainingPairs(void);
void showTrainingPairs(void);
void showOutputs(void);
void computeOutputDeltaValues(int sample);
void computeHiddenDeltaValues(void);
void updateParameters(float eta);
void saveTrainedNetworkParameters(int counter);

float nonLinearFunction(float sum, float threshold);
float derivativeNonlinearFunction(float sum);


// GLOBAL VARIABLES

struct WEIGHTS
{
    float w[LIMIT][LIMIT];
    float w1[LIMIT][LIMIT];
    float w2[LIMIT][LIMIT];
}w[5];

struct VECTOR
[
    float out[LIMIT];     //output values
    float del[LIMIT];     //delta values
    float thr[LIMIT];     //threshold values
    float SJ[LIMIT];
}s[5];

int L, layer [LIMIT+1];
int PAIRS, BACKUP = 1E3;
float far *pi, *po;
char train_pairs[20];
char file_name[20];
FILE *fp, *fer;

void main()
{
    int counter, sample, k, i, choi;
    float cluster_error, old_cluster_error, eta = ETA, error_sample;
    
    pi = (float far * ) farmalloc ((long) (5000));
    if(pi == NULL)
    {printf ("\n Out of Memory... \n"); exit(1);}
    
    clrscr();
    getStructureInfo();
    assignInitialValues();
    loadTrainingPairs();
    showTrainingPairs();
    
    if((fer = fopen ("ERROR.M", "w")) == NULL)
    {
        fprintf(stderr, " ERROR.M File could not open\n");
        exit(1);
    }
    
    clrscr();
    old_cluster_error = 0;
    for(counter = 0; counter <= ITERATION; counter++)
    {
        cluster_error = 0;
        for(sample = 1; sample <= PAIRS; sample++)
        {
            error_sample = 0;
            for(k = 1; k <= layer[0]; k++)
                s[0].out[k] = x(k, sample);
            computeForwardValues();
            showOutputs(sample);
            
            for(i = 1; i <= layer[L+1]; i++)
                error_sample = error_sample + 0.5 * pow ((d(i, sample)-s[L+1].out[i]), 2);
                
            cluster_error += error_sample;
            computeOutputDeltaValues(sample);
            computeHiddenDeltaValues();
            updateParameters(eta);
        }
        
        fprintf(fer, " %d  %f \n", counter, cluster_error);
        
        //Step Size Adaptation
        if(cluster_error - old_cluster_error < 0) 
            eta += GAMMA;
        else if(cluster_error - old_cluster_error > 0)
            eta = (1-BETA)*eta;
        else{//NO CHANGE}
        
        old_cluster_error = cluster_error;
        
        gotoxy(20, 20);
        printf("Cluster Error = %f ", cluster_error);
        
        gotoxy(20, 21);
        printf("Number of Cluster Transitions = %d", counter);
    }
    
    // SAVING NETWORK PARAMETERS (OUTPUT)
    if( counter < ITERATION && ( kbhit() || cluster_error <=  TOLERANCE))
    {
        saveTrainedNetworkParameters(counter);
        gotoxy(20,23);
        printf("Latest Status Saved.");
        gotoxy(12,24);
        printf("Check the <&s> file for details.\n, file_name");
        break;
    }
    
    // BACKUP NETWORK PARAMETERS
    if( (counter%BACKUP) == (BACKUP -1))
    {
        gotoxy(20,23);
        printf("Saving automatically...      ");
        saveTrainedNetworkParameters(counter);
        gotoxy(20,23);
        printf("  ");
    }
}

    fclose(fer);
    farfree(pi);
    farfree(po);
}

//**************************************************************************
void getStructureInfo(void)
{
    int i;
    printf("\n\n Training Pairs File Name: ");
    scanf("%s", train_pairs);
    printf("\n\n Enter Number of Hidden Layers: ");
    scanf("%d", &L);

    // (L+1)th layer is the output layer
    // Zero layer is the input layer
   
   printf("Enter Number of Input Neurons: ");
   scanf("%d", &layer[0]);
   printf("Enter Number of Output Neurons: ");
   scanf("%d", &layer[L+1]);
   
   for(i=1; i<=L; i++)
   {
       printf("Enter Number of Neurons at &d. Layer: ", i);
       scanf("%d", &layer[i]);
   }
   gotoxy(1,16);
   printf("Enter File Name of Trained Network Parameters to Save: ");
   scanf("%s", file_name);
}

//**************************************************************************
void assignInitialValues(void)
{
    int k, i, j;
    
    randomize();
    for(k=0; k<=L; k++)
    {
        for(i=1; i<=layer[k+1]; i++)
        {
            for(j=1; j<=layer[k]; j++)
            {
                W[k].w[i][j] = ((float) (rand()%10000))/10000-0.5;
                W[k].w1[i][j] = ((float) (rand()%10000))/10000-0.5;
                W[k].w2[i][j] = ((float) (rand()%10000))/10000-0.5;
            }
            s[k].thr[i] = ((float) (rand()%10000))/100000-0.5;
        }
    }
}

//**************************************************************************
float nonLinearFunction(float sum, float threshold)
{
    float value;
    value = tanhl(sum - threshold);         //tanhl
    return(value);
}

//**************************************************************************
void computeForwardValues(void)
{
    int i, j, k;
    float sum;
    
    for(k=0; k<=L; k++)
    {
        for(i=1; i<=layer[k+1]; i++)
        {
            sum = 0;
            for(j=1; j<=layer[k]; j++)
            {
                sum = sum + W[k].w[i][j] * s[k].out[j];
            }
            s[k+1].SJ[i] = sum;
            if(k!=L) 
                s[k].out[i] = nonLinearFunction(s[k+1].SJ[i], s[k+1].thr[i]);
            else
                s[k+1].out[i] = sum - s[k+1].thr[i];
        }
    }
}

//**************************************************************************
void derivativeNonlinearFunction(float sum)
{
    float derivative;
    derivative = (1 - sum*sum);
    return(derivative);
}

//**************************************************************************
void loadTrainingPairs(void)
{
    int i, j;
    float temp;
    
    // Load Training Pairs to Memory
    // Every Single Consecutive Layer[0] -elements Groups is one Train
    // Constitute Input of Pair 
    // x(<which input of network>,<which sample(in order)>)
    // d(<which output of network>, <which sample(in order)>)
    
    if((fp = fopen( train_pairs, "r")) == NULL)
    {
        fprintf(stderr, "File Can Not Open: %s\n", train_pairs);
        exit(1);
    }
    fscanf(fp, "%d", &PAIRS);
    
    for(j=1; j<=PAIRS; j++)
    {
        for(i=1; i<=layer[0]; i++)
        {
            fscanf(fp, "%f", &temp);
            d(i, j) = temp;
        }
    }
    fclose(fp);
}

//**************************************************************************
void showTrainingPairs(void)
{
    int i, j;
    
    clrscr();
    for(j=1; j<=PAIRS; j++)
    {
        for(i=1; i<=layer[0]; i++)
        {
            printf("x(%d, %d) = %f ", i, j, x(i, j));
        }
        for(i=1; i<=layer[L+1]; i++)
        {
            print("d(%d, %d) = %f\n", i, j, d(i, j));
        }
    }
    getch();
}

//**************************************************************************
void showOutputs(int sample)
{
    int i, j;
    
    for(i=1; i<=PAIRS; i++)
    {
        gotoxy(5, 1 + sample%15);
        for(j=1; j<=layer[L+1]; j++)
        {
            printf("out = %f", s[L+1].out[1]);
        }
    }
}

//**************************************************************************
void computeOutputDeltaValues(int sample)
{
    int j;
    
    for(j=1; j<=layer[L+1]; j++)
    {
        s[L+1].del[j] = ( d(j, sample)-s[L+1].out[j] );
    }
}

//**************************************************************************
void computeHiddenDeltaValues(void)
{
    float derivative, sum;
    int l, j, k;
    
    // Delta Values for Hidden Layers
    // k index: Layer Order
    
    for(k=L; k>=1; k--)
    {
        for(j=1; j<=layer[k]; j++)
        {
            sum = 0;
            for(l=1; l<=layer[k+1]; l++)
            {
                sum = sum + s[k+1].del[l] * W[k].w[l][j];
            }
            derivative = derivativeNonlinearFunction(s[k].out[j]);
            s[k].del[j] = sum * derivative;
        }
    }
}

//**************************************************************************
void updateParameters(float eta)
{
    int kk, i, j;
    float DIFF, MOMENTA;
    
    // Apply Parametre Updating Code
    for(kk=0; kk<=L; kk++)  // kk -> Scans Interlayer Weight Matrixes
    {
        for(i=1; i<=layer[kk+1]; i++)   // i -> Scans Neurons Close to the Output
        {
            for(j=1; j<=layer[kk]; j++)    // j -> Scans Neurons Close to the Input
            {
                DIFF = eta * s[kk+1].del[i] * s[kk].out[j];
                W[kk].w2[i][j] = W[kk].w1[i][j];
                W[kk].w1[i][j] = W[kk].w[i][j];
                MOMENTA = MOMENTUM * (W[kk].w1[i][j] - W[kk].w2[i][j]);
                W[kk].w[i][j] += (DIFF + MOMENTA);
            }
            s[kk+1].thr[i] += eta * s[kk+1].del[i] * (-1);
        }
    }
}

//**************************************************************************
void saveTrainedNetworkParameters(int counter)
{
    FILE *fw;
    int i, j, matrix;
    
    if((fw = fopen( file_name, "w")) == NULL)
    {
        fprintf(stderr, "File Couldn't Open: %s\n", file_name);
        exit(1);
    }
    fprintf(fw, "\n     *** TRAINED NETWORK PARAMETERS ***      \n\n");
    fprintf(fw, "   Number of Hidden Layer  ->  %d\n", L);
    fprintf(fw, "   Input Vector Size  ->  %d\n", layer[0]);
    fprintf(fw, "   Output Vector Size  ->  %d\n", layer[L+1]);
    
    for(i=1; i<=L; Ä°++)
    {
        fprintf(fw, "   Number of Neurons at [%d]. Hidden Layer  ->  %d\n", i, layer[i]);
    }
    
    fprintf(fw, "\n    ==== WEIGHT MATRIXES ====\n\n");
    
    for(matrix=0; matrix<=L; matrix++)
    {
        for(i=1; i<=layer[matrix+1]; i++)
        {
            for(j=1; j<=layer[matrix]; j++)
            {
                fprintf(fw, "   w %f    ", W[matrix].w[i][j]);
            }
            fprintf(fw, "   [ %f ]\n", s[matrix+1].thr[i]);
        }
        fprintf(fw, "\n\n")
    }
    fprintf(fw, " Number of Cluster Transition = %d", counter);
    fclose(fw);
}
